name: Local LLM PR Summary

on:
  pull_request:
    types: [opened, synchronize]

permissions:
  contents: read
  pull-requests: write

jobs:
  generate-summary:
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/ggml-org/llama.cpp:full

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install build dependencies
        run: |
          apt-get update && apt-get install -y cmake build-essential git libcurl4-openssl-dev

      - name: Clone llama.cpp
        run: |
          git clone https://github.com/ggml-org/llama.cpp.git /llama.cpp

      - name: Build llama.cpp
        run: |
          cd /llama.cpp
          cmake -B build -DLLAMA_BUILD_EXAMPLES=ON
          cmake --build build --config Release -j$(nproc)

      - name: Download model (Mistral 7B)
        run: |
          mkdir -p /models/
          curl -L https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf \
            -o /models/mistral.gguf

      - name: Locate `llama-cli` binary
        id: locate-cli
        run: |
          echo "Listing all executables in build dir..."
          find /llama.cpp/build -type f -executable -exec ls -l {} \;

          CLI_PATH=$(find /llama.cpp/build -type f -name llama-cli -executable | head -n 1)
          echo "Found llama-cli binary at: $CLI_PATH"
          if [ -z "$CLI_PATH" ]; then
            echo "‚ùå llama-cli binary not found"
            exit 1
          fi
          echo "main-path=$CLI_PATH" >> $GITHUB_OUTPUT

      - name: Generate diff summary
        id: llm-summary
        run: |
          cd "$GITHUB_WORKSPACE"
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          git status || { echo "‚ùå Not a git repo"; exit 1; }

          BASE_SHA=${{ github.event.pull_request.base.sha }}
          HEAD_SHA=${{ github.event.pull_request.head.sha }}

          # Extract and trim the unified diff (safe for LLM input)
          DIFF_CONTENT=$(git diff --unified=0 "$BASE_SHA" "$HEAD_SHA" | head -c 5500)

          # Craft a refined prompt for better summarization
          PROMPT="ü§ñ Local LLM Pull Request Summary
          [INST]
          Analyze the following code changes with a focus on:
          1. **Main Purpose** ‚Äì What does this PR aim to achieve?
          2. **Key Code Changes** ‚Äì List the major technical changes.
          3. **Impact** ‚Äì Any functional, performance, or architectural implications.

          Only summarize the code changes ‚Äî do NOT include raw diffs, workflow steps, or unrelated metadata. Keep it clear, technical, and under 500 words.

          ${DIFF_CONTENT}
          [/INST]"

              MAIN_BIN="${{ steps.locate-cli.outputs.main-path }}"
              if [ ! -x "$MAIN_BIN" ]; then
                echo "‚ùå llama-cli binary not executable or missing at $MAIN_BIN"
                exit 1
              fi

              SUMMARY=$("$MAIN_BIN" \
                -m /models/mistral.gguf \
                --temp 0.7 \
                --ctx-size 2048 \
                --keep -1 \
                --n-predict 512 \
                --prompt "$PROMPT")

              # Bonus formatting: Wrap summary in a section with emoji and Markdown
              echo "SUMMARY<<EOF" >> $GITHUB_OUTPUT
              echo -e "## üìù PR Summary by Local LLM\n\n$SUMMARY" >> $GITHUB_OUTPUT
              echo "EOF" >> $GITHUB_OUTPUT

              echo "$SUMMARY"

      - name: Post summary
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body: |
            ### ü§ñ Local LLM Analysis (Mistral 7B)

            ${{ steps.llm-summary.outputs.SUMMARY }}

            *Generated on GitHub Actions runner*
          edit-mode: replace
